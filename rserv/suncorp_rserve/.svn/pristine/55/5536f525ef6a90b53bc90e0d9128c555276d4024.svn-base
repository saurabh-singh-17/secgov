#---------------------------------------------------------------------------------------#
#--                                                                                   --#
#--  Project Name:   Text Mining                                                      --#
#--  Task :          Task 1: Data Handling                                            --#
#--  Sub-Task:       1.1 Appending the pre-processed data to the original file        --#
#--  version :       1.0 date: 14/03/2012 author: Gaurav Jain                          --#
#---------------------------------------------------------------------------------------#


#Parameters required
#-----------------------------------------------------------------
# input_path="C:/Documents and Settings/shankar.jha/My Documents"
# output_path="C:/Documents and Settings/shankar.jha/My Documents"
# varName=c("tweettext","comment")
# newVarName=c("variable_1","variable_2")
# removeEmails  = TRUE
# removeUrls = TRUE
# removePhoneNumber = TRUE
# removeNumber = TRUE
# removePunctuations = TRUE
# removeStopwords = TRUE
# stripWhitespaces = TRUE
# stemDoc = TRUE

#Configurations to load rJava library
#====================================

# #Reading java path from the system paths
# #---------------------------------------
# jPath <- readChar(jpath.location, file.info(jpath.location)$size)
# 
# #Adding client to JPath as jvm.dll exists inside that folder is used to load rJava
# #---------------------------------------------------------------------------------
# jPath.client=paste(jPath,'\\client',sep='')
# 
# #Checking for jre client or jre server
# #-------------------------------------
# if(file.exists(jPath.client)){
#   jPath <- jPath.client
# }else{
#   jPath <- paste(jPath,'\\server',sep='')
# }
# 
# #Setting temporary java client/server path
# #-----------------------------------------
# try(Sys.setenv('path'=paste(Sys.getenv('path'),jPath,sep=';')),silent=TRUE)
# 
# #Libraries required
# #-----------------------------------------------------------------
# is.installed <- function(pkg){ is.element(pkg, installed.packages()[,1])}
# if(!is.installed('RTextTools')){
#   install.packages('RTextTools',repo="http://lib.stat.cmu.edu/R/CRAN")
# }
# 
# if(!is.installed('Snowball')){
#   install.packages('Snowball',repo="http://lib.stat.cmu.edu/R/CRAN")
# }
library('SnowballC')
library('tm')
library('openNLP')
library('openNLPmodels.en')
library('lda')
library('XML')
library('slam')
library('stringr')
library('wordcloud')
library('RColorBrewer')
library('rJava')
library('RTextTools')
library('Snowball')


#---------------------------------------------------------------------------------------
# Loading the data
#---------------------------------------------------------------------------------------
load(paste(input_path,"/dataworking.RData",sep=""))
grp_no <- as.numeric(grp_no)
index <- 1:nrow(dataworking)
current.level <- NULL
if (grp_no != 0) {
  index <- which(dataworking[, paste("grp", grp_no, "_flag", sep="")] == grp_flag)
  
  # Getting the current level
  temp <- read.table(file=paste(input_path, "/", grp_no, "/byvar.csv", sep=""),
                     header=T,
                     sep=",",
                     stringsAsFactors=F)
  index.col <- which(!(colnames(temp) %in% c("KEY_NAME", "FLAG")))
  index.row <- which(temp$FLAG == grp_flag)
  current.level <- paste("_P", grp_no, "_", paste(temp[index.row, index.col], collapse="_"), sep="")
  current.level <- gsub(pattern="[^[:alnum:]_]", replacement="_", x=current.level)
}
#---------------------------------------------------------------------------------------



#---------------------------------------------------------------------------------------
# function : datacleansing
#---------------------------------------------------------------------------------------
dataHandling <- function (x,
                          removeEmails  = FALSE,
                          removeUrls = FALSE,
                          removePhoneNumber = FALSE,
                          removeNumber = FALSE,
                          removePunctuations = FALSE,
                          removeStopwords = FALSE,
                          stripWhitespaces = FALSE,
                          stemDoc = FALSE) {
  
  
  
  # Regular expressions to match 1. Email, 2. URL, 3. Phone number
  #---------------------------------------------------------------
  
  email.expression <- "[A-Za-z0-9-]+[.A-Za-z0-9-]*@[A-Za-z0-9-]+(\\.com|\\.co.in|\\.net|\\.org|\\.info|\\.edu|\\.mil|\\.gov|\\.biz|\\.ws|\\.us|\\.tv|\\.cc|\\.aero|\\.arpa|\\.coop|\\.int|\\.jobs|\\.museum|\\.name|\\.pro)|\\.travel|\\.nato)"
  url.expression <- "(http://|https://|www.)[[:alnum:]~!#$%&+-=?,:/;._]*"
  phonenumber.expression <- "\\+?(\\d{2,3})[- ]?\\(?(\\d{3,5})\\)?[- ]?(\\d{3,5})[- ]?(\\d{4})?"
  
  # To read data from a single csv file and create a dataset of required column
  #----------------------------------------------------------------------------
  
    
  corpus <- as.character(x)
 
  # To remove emails from dataset
  #----------------------------------------------------------------------------
  
  
  if(removeEmails) {
    corpus <- gsub(email.expression, ' ', corpus, ignore.case=T)
  }
  
  # To remove urls from dataset
  #----------------------------------------------------------------------------
  
  if(removeUrls) {
    corpus <- gsub(url.expression, ' ', corpus, ignore.case=T)
  }
  
  # To remove phone numbers from dataset
  #----------------------------------------------------------------------------
  
  if(removePhoneNumber) {
    corpus <- gsub(phonenumber.expression, ' ', corpus, ignore.case=T, perl=TRUE)
  }
  
  # To remove duplicate words from a row
  #----------------------------------------------------------------------------
  
  if (as.logical(removeDuplicates)) {
    # This code was written by Aishwarya :)
    temp_splitbyspace <- strsplit(x=corpus, split=" ")
    temp_splitbyspace <- lapply(X=temp_splitbyspace, FUN=unique)
    temp <- unlist(lapply(X=temp_splitbyspace, FUN=paste, collapse=" "))
    corpus <-  temp
    rm("temp_splitbyspace")
  }
  
  # To covert dataset into a Corpus; required for executing 'tm_map' functions
  #----------------------------------------------------------------------------
  
  corpus <- Corpus(VectorSource(corpus))
  
  # To remove stopwords from corpus
  #----------------------------------------------------------------------------
  
  if(removeStopwords) {
    corpus <- tm_map(corpus, removeWords, stopwords("english"))
  }
  
  # To remove numbers from corpus
  #----------------------------------------------------------------------------
  
  if(removeNumber) {
    corpus <- tm_map(corpus, removeNumbers)
  }
  
  # To remove punctuations from corpus
  #----------------------------------------------------------------------------
  
  if(removePunctuations) {
    corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_dashes = TRUE)
  }
  
  # To stem words present in the corpus data
  #----------------------------------------------------------------------------
  
  if(stemDoc) {
    corpus <- tm_map(corpus, stemDocument)
  }
  
  # To remove additional whitespaces from corpus
  #----------------------------------------------------------------------------
  
  if(stripWhitespaces) {
    corpus <- tm_map(corpus, stripWhitespace)
  }
  
  # Converting corpus into a vector
  #----------------------------------------------------------------------------
  x <- unlist(corpus[1:length(x)])
  x <- as.character(x)
  
  return(x)
  
}

prefix <- NULL
newvarnames <- NULL
if (exists("newVarName") & newVarName != "") {
  prefix <- newVarName
  rm("newVarName")
  newvarnames <- paste(prefix, current.level, "_", varName, sep="")
}

if (!(is.null(newvarnames))) {
  
  for(i in 1:length(varName)){
    
    temp <- as.character(dataworking[, varName[i]])
    dataworking[,varName[i]]<-as.character(dataworking[,varName[i]])
    dataworking[index, varName[i]]<-iconv(dataworking[index, varName[i]],from = "WINDOWS-1252",to="UTF-8")
    
    temp[index] <- dataHandling (x=dataworking[index, varName[i]],
                                 removeEmails,
                                 removeUrls,
                                 removePhoneNumber,
                                 removeNumber,
                                 removePunctuations,
                                 removeStopwords,
                                 stripWhitespaces,
                                 stemDoc)
    
    dataworking[, newvarnames[i]] <- temp
    
  }
  
}

# To remove customWords
#----------------------------------------------------------------------------

if (as.logical(removeCustomWords)) {
  
  if (!(is.null(newvarnames))) {
    varName_forCustomWords <- newvarnames
  } else {
    varName_forCustomWords <- varName
  }
  
  customWords <- c(customWords,paste(" ", customWords, sep=""), 
                   paste(customWords, " ", sep=""), 
                   paste(" ", customWords, " ", sep=""),
                   paste("^", customWords, "$", sep=""))
  
  for (i in varName_forCustomWords) {
    for (j in customWords) {
      dataworking[, i] <- gsub(pattern=j,
                               replacement="",
                               x=dataworking[, i],
                               ignore.case=T)
    }
  }
}

save(dataworking,file=paste(input_path,"/dataworking.RData",sep=""))
write.csv(dataworking,
          file=paste(input_path, "/dataworking.csv", sep=""),fileEncoding = "WINDOWS-1252",
          row.names=F, quote=F)

#writing the completed text at the output location
#-----------------------------------------------------------------

write(newvarnames, file = paste(output_path, "TEXT_DATA_HANDLING_COMPLETED.TXT", sep="/"))