#---------------------------------------------------------------------------------------#
#--                                                                                   --#
#--  Project Name:   Text Mining                                                      --#
#--  Task :          Task 1: Text Categorization                        			        --#
#--  Sub-Task:       1.1 Model Training                                               --#
#--  Sub-Task:       1.2 Model Validation                                             --#
#--  version :       1.0 date: 04/04/2012 author: Gaurav Jain		          		        --#
#---------------------------------------------------------------------------------------#



# Defining some custom functions
# 1. To install required packages
#---------------------------------------------------------------------------------------

is.installed <- function(pkg){ is.element(pkg, installed.packages()[,1])}


mlTrainValidate <- function(filePath, fileName, indVarName, depVarName,
                    modelLoc, matrixName, modelName,
                    modelStatsLoc, docSummaryReport, lblSummaryReport, algoSummaryReport, predActReport,
                    predReport, actualReport,
                    isInSampleValid = FALSE, isOutSampleValid = FALSE,
                    outSampleLoc, trainRatio = 80,
                    isSVM = FALSE, isRF = FALSE, isMAXENT = FALSE, isGLM = FALSE,
                    minWordLen = 3, minDocumentFreq = 1, ifTf = FALSE, ifTfIdf = FALSE,
                    removePunctuations = FALSE, removeNumber = FALSE,
                    stripWhitespaces = FALSE, stemDoc = FALSE, isLower = FALSE,
                    remStopwords = FALSE){


  if(!is.installed('RTextTools')){
    install.packages('RTextTools',repo="http://lib.stat.cmu.edu/R/CRAN")
    library('RTextTools')
  }

  require('RTextTools')

  # To read data from a single csv file and create a dataset of required column
  #----------------------------------------------------------------------------

  fileLoc <- paste(filePath,fileName,sep="\\")

  tmDataSet <- read_data(fileLoc,type="csv")

  varIndexInd <- which(colnames(tmDataSet)==indVarName)
  varIndexDep <- which(colnames(tmDataSet)==depVarName)


  # To create training and validation sets
  #----------------------------------------------------------------------------

  dataSize <- nrow(tmDataSet)

  if(isInSampleValid) {
    trainingSize = round((trainRatio/100)*dataSize)
    validationSize = dataSize - trainingSize

    data_training <- tmDataSet[sample(1:dataSize,size=trainingSize,replace=FALSE),]

    rowIndex <- row.names(data_training)
    delRowIndex <- which(row.names(tmDataSet) %in% rowIndex)
    data_validation <- tmDataSet[-delRowIndex,]

    trainValidData <- rbind(data_training,data_validation)

    indData <- trainValidData[,varIndexInd]
    depData <- trainValidData[,varIndexDep]

  }

  # To create a matrix from the dataset
  #----------------------------------------------------------------------------

  if(ifTf) {
  	weighting = weightTf
  } else if(ifTfIdf) {
  	weighting = weightTfIdf
  }

  matrix <- create_matrix(indData,language="english",
                          minWordLength = minWordLen,minDocFreq = minDocumentFreq,ngramLength = 0,
                          removeNumbers = removeNumber,stemWords = stemDoc,
                          removePunctuation = removePunctuations,stripWhitespace = stripWhitespaces,
                          toLower = isLower,removeStopwords = remStopwords,
                          weighting,removeSparseTerms = .998)


  # To create a corpus of independent and dependent data
  #----------------------------------------------------------------------------

  corpus <- create_corpus(matrix,depData,trainSize=1:trainingSize,
                          testSize=(trainingSize+1):dataSize,virgin=FALSE)


  # To train on selected algorithms
  #----------------------------------------------------------------------------

  algorithms = NULL

  if(isSVM) {
    algorithms="SVM"
  }
  if(isRF) {
    algorithms = c(algorithms, "RF")
  }
  if(isMAXENT) {
    algorithms = c(algorithms, "MAXENT")
  }
  if(isGLM) {
    algorithms = c(algorithms, "GLMNET")
  }

  models <- train_models(corpus, algorithms)


  # To save generated matrix and model
  #----------------------------------------------------------------------------

  matrixPath <- paste(modelLoc,matrixName,sep="\\")
  modelPath <- paste(modelLoc,modelName,sep="\\")

  save(matrix,file=matrixPath)
  save(models,file=modelPath)


  # To create analytics reports
  #----------------------------------------------------------------------------

  results <- classify_models(corpus, models)

  analytics <- create_analytics(corpus, results)

  Statistic = c()
  Value = c()

  Statistic = c(Statistic, "Consensus Accuracy")
  Value = c(Value, recall_accuracy(analytics@document_summary$MANUAL_CODE,
                  analytics@document_summary$CONSENSUS_CODE))

  if(isSVM) {
    Statistic = c(Statistic, "SVM Accuracy")
    Value = c(Value, recall_accuracy(analytics@document_summary$MANUAL_CODE,
                                   analytics@document_summary$SVM_LABEL))
  }
  if(isRF) {
    Statistic = c(Statistic, "Random Forest Accuracy")
    Value = c(Value, recall_accuracy(analytics@document_summary$MANUAL_CODE,
                    analytics@document_summary$FORESTS_LABEL))
  }
  if(isMAXENT) {
    Statistic = c(Statistic, "Maximum Entropy Accuracy")
    Value = c(Value, recall_accuracy(analytics@document_summary$MANUAL_CODE,
                    analytics@document_summary$MAXENTROPY_LABEL))
  }
  if(isGLM) {
    Statistic = c(Statistic, "GLMNET Accuracy")
    Value = c(Value, recall_accuracy(analytics@document_summary$MANUAL_CODE,
                    analytics@document_summary$GLMNET_LABEL))
  }

  ModelStats = data.frame(Statistic,Value)

  # To write the model stats summary report in a csv
  #----------------------------------------------------------------------------

  write.csv(ModelStats, file = modelStatsLoc, append = FALSE, col.names = TRUE,
            row.names = FALSE)


  # To write the model stats detailed reports in csvs
  #----------------------------------------------------------------------------

  write.csv(analytics@label_summary, lblSummaryReport)
  write.csv(analytics@algorithm_summary, algoSummaryReport)

  Data_Validation = data.frame(data_validation[,varIndexInd])
  colnames(Data_Validation) = "Data_Validation"

  write.csv(cbind(Data_Validation, analytics@document_summary),
            docSummaryReport)


  # To calculate the % category share
  #----------------------------------------------------------------------------

  predicted_Category <- analytics@document_summary$CONSENSUS_CODE
  actual_Category <- analytics@document_summary$MANUAL_CODE

  predTable = table(predicted_Category)
  predTable = (predTable/sum(predTable))*100

  actualTable = table(actual_Category)
  actualTable = (actualTable/sum(actualTable))*100

  write.csv(predTable, file = predReport, append = FALSE, col.names = TRUE,
            row.names = FALSE)
  write.csv(actualTable, file = actualReport, append = FALSE, col.names = TRUE,
            row.names = FALSE)


  # To write the predicted vs actual report in a csv
  #----------------------------------------------------------------------------

  write.csv(cbind(Data_Validation,analytics@document_summary$CONSENSUS_CODE,
                  analytics@document_summary$MANUAL_CODE),predActReport)


  # To clear all variable used
  #----------------------------------------------------------------------------

  rm(list=c("filePath","fileName","fileLoc", "varName","varIndexInd","varIndexDep",
            "indData","depData","algorithms","matrixLoc","modelLoc",
            "matrix","corpus","models","traininig_size","predicted_Category","actual_Category",
            "predTable","actualTable","ModelStats","results","analytics","Statistic","Value"))

}